{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4c422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccad082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d077bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    The MLP layer used in between multi - head attention blocks to capture and retain information between tokens. \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b95f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"  \n",
    "    The positional embedding of the input tokens, to hold information about where the token is in the input sequence. Added with the embedding vector of the token.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)   # A tensor filled with zeros, which will be populated with positional encodings.\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)  # A tensor containing the position indices for each position in the sequence.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))  # A term used to scale the position indices in a specific way.\n",
    "        \n",
    "        # The sine function is applied to the even indices and the cosine function to the odd indices of pe.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Added as a buffer so it is added to the model's state but not a trainable parameter\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a7431df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\" \n",
    "    The encoding layer of the transformer: takes the input sequence of tokens and extracts the meaning and context to output a contextualized matrix of embedded tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)     # Multi head attention blocks, with d_model multi-head blocks and num_heads self attention mechanisms in each multi head block\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)  # MLP layer applied after each multi head block\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Normalization for the outputs of the MLP and Multi head blocks\n",
    "        self.norm2 = nn.LayerNorm(d_model)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d360136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\" \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        \"\"\" \n",
    "        x: The input to the decoder layer.\n",
    "        enc_output: The output from the corresponding encoder (used in the cross-attention step).\n",
    "        src_mask: Source mask to ignore certain parts of the encoder's output.\n",
    "        tgt_mask: Target mask to ignore certain parts of the decoder's inp  \n",
    "\n",
    "        Steps:\n",
    "\n",
    "        Self-Attention on Target Sequence: The input x is processed through a self-attention mechanism.\n",
    "        Add & Normalize (after Self-Attention): The output from self-attention is added to the original x, followed by dropout and normalization using norm1.\n",
    "        Cross-Attention with Encoder Output: The normalized output from the previous step is processed through a cross-attention mechanism that attends to the encoder's output enc_output.\n",
    "        Add & Normalize (after Cross-Attention): The output from cross-attention is added to the input of this stage, followed by dropout and normalization using norm2.\n",
    "        Feed-Forward Network: The output from the previous step is passed through the feed-forward network.\n",
    "        Add & Normalize (after Feed-Forward): The feed-forward output is added to the input of this stage, followed by dropout and normalization using norm3.\n",
    "        Output: The processed tensor is returned as the output of the decoder layer.\n",
    "        \"\"\"\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6474505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, num_classes=None, classification=False):\n",
    "\n",
    "        \"\"\" \n",
    "        src_vocab_size: Source vocabulary size.\n",
    "        tgt_vocab_size: Target vocabulary size.\n",
    "        d_model: The dimensionality of the model's embeddings.\n",
    "        num_heads: Number of attention heads in the multi-head attention mechanism.\n",
    "        num_layers: Number of layers for both the encoder and the decoder.\n",
    "        d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
    "        max_seq_length: Maximum sequence length for positional encoding.\n",
    "        dropout: Dropout rate for regularization.\n",
    "\n",
    "        num_classes and classification: parameters that allow the transformer archietcure to adapt to either classifciation NLP or generation tasks (hopefully easily)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  # Embedding layer for the source sequence.\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)  # Embedding layer for the target sequence.\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length) # Positional encoding component.\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])   # A list of encoder layers.\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])   # A list of decoder layers.\n",
    "        self.classification = classification\n",
    "        if not classification:\n",
    "            out_dim = tgt_vocab_size\n",
    "        else:\n",
    "            out_dim = num_classes\n",
    "        self.fc = nn.Linear(d_model, out_dim)    # Final fully connected (linear) layer mapping to target vocabulary size.\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "\n",
    "    def generate_mask(self, src, tgt=None):\n",
    "        \"\"\" \n",
    "        This method is used to create masks for the source and target sequences, ensuring that padding tokens are ignored and that future tokens are not visible during training for the target sequence.\n",
    "        \"\"\"\n",
    "        # If we don't want a tgt mask, \n",
    "        if tgt is not None:\n",
    "            tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "            seq_length = tgt.size(1)\n",
    "            nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "            tgt_mask = tgt_mask & nopeak_mask\n",
    "        else:\n",
    "            tgt_mask = None\n",
    "\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt=None):\n",
    "        \"\"\" \n",
    "        Final output is the decoded tensor representing the models prediction for the next token in the sequence. \n",
    "        \"\"\"\n",
    "\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        if self.classification:\n",
    "            x = enc_output[:, 0, :]\n",
    "            return self.fc(x)\n",
    "        \n",
    "       \n",
    "\n",
    "        # If used for generation, include the decoder layer\n",
    "        if tgt:\n",
    "            tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "            dec_output = tgt_embedded\n",
    "            for dec_layer in self.decoder_layers:\n",
    "                dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "            output = self.fc(dec_output)\n",
    "        \n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projects_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
