{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch\n",
    "\n",
    "Below code is a step by step coding tutorial from the following website, which is a basic transformer model following the architecture set in 'Attention is all you need' research paper. The transformer is trained on the imdb dataset from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nStill have issues with the training loop and data handling. When running in training, nothing is output, perhaps due to going through a batch with the \\ntraining data loader being too big, not sure. \\n\\nCheck out YT video currently watching to try and change training, or use hugging face. Would like to write at least one full trinaing loop, issue most likely in how data being \\nhandled and batched.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Still have issues with the training loop and data handling. When running in training, nothing is output, perhaps due to going through a batch with the \n",
    "training data loader being too big, not sure. \n",
    "\n",
    "Check out YT video currently watching to try and change training, or use hugging face. Would like to write at least one full trinaing loop, issue most likely in how data being \n",
    "handled and batched.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam\\anaconda3\\envs\\Projects_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 5e-5\n",
    "EPOCHS = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "block_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam\\anaconda3\\envs\\Projects_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sam\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "------------------ Preprocessing ------------------\n",
    "\n",
    "1. Split data into training/validation datasets\n",
    "2. Text preprocessing to get it into a nice format\n",
    "    - Remove trailing whitespaces, any encoding issues, lowercasing for models that rely on specific casing of characters\n",
    "3. Tokenize the data\n",
    "    - pick tokenizers for the specific model or architecture being used\n",
    "        - Word Pieces (BERT)\n",
    "        - Byte-Pair encoding (GPT)\n",
    "        - SentencePiece (P5)\n",
    "4. Depending on the task, may need specific inputs from the tokenizer output\n",
    "     - Classification: input ids, attention masks, label\n",
    "     - translation: input ids, decoder input ids, labels\n",
    "     - text generation: just input ids and maybe a promnpt\n",
    "\n",
    "5. Create a dataset or a data loader ovject (pyTorch) if already in dataset form (example loaded from Hugging Face)\n",
    "\"\"\"\n",
    "\n",
    "# Dataset already split into training data, no validation for this small dataset\n",
    "# No need to remove or clean dataset as it is already fine from hugging face\n",
    "\n",
    "# Import tokenizer from huggingface\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(imdb['train'][0]['text'])\n",
    "\n",
    "# returns: input ids: the numbers representing the tokens in the text\n",
    "# token type ids: the sequence the token belongs to, if more than 1\n",
    "# attention mask: should it be masked or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:14<00:00, 1745.08 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:15<00:00, 1611.14 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:31<00:00, 1584.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# def tokenization(example):\n",
    "#     # Tokenizes the dataset, typically works with 'text' field for input text\n",
    "#     return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "\n",
    "# # Create the mappings and format for the data splits so they can be loaded into a pyotrch data loader\n",
    "# train_data = imdb['train'].map(tokenization, batched=True)\n",
    "# train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"])\n",
    "\n",
    "# val_data = imdb['test'].map(tokenization, batched=True)\n",
    "# val_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"])\n",
    "\n",
    "# test_data = imdb['unsupervised'].map(tokenization, batched=True)\n",
    "# test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"])\n",
    "\n",
    "# # Define a DataLoader for batching during training\n",
    "# train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)    # dynamically pad the tokens\n",
    "accuracy = evaluate.load(\"accuracy\")    # accracy metric from hugging face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\" \n",
    "    Apply the accuracy metric to the validate predictions\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "------------------ Training Loop ------------------\n",
    "\n",
    "Very simple to train, can pass in the batch of the data to the transformer, check dimensions are the same for batch that the mdoel can handle\n",
    "Get the outputs, calculate the loss and step backwards the loss function and the optimizer to update the weights in the model. \n",
    "\n",
    "Logits can be calculated from the output predictions and can be used to show accuracy measurements, etc.\n",
    "\"\"\"\n",
    "\n",
    "# Parameters based on IMDB dataset\n",
    "src_vocab_size = tokenizer.vocab_size  # 30522 for 'bert-base-uncased'\n",
    "tgt_vocab_size = tokenizer.vocab_size  # Same as source for classification\n",
    "d_model = 512  # Standard model dimension\n",
    "num_heads = 8  # Multi-head attention heads\n",
    "num_layers = 6  # Number of encoder/decoder layers\n",
    "d_ff = 2048  # Feed-forward network size\n",
    "max_seq_length = 512  # Max length of sequences\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Initialize the model\n",
    "transformer_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, num_classes=2, classification=True)\n",
    "transformer_model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "# ---------- Issues/to do  ---------- #\n",
    "\n",
    "# The train loader and how this unpacks data\n",
    "# the batching of the data to be trained and the forward pass parameters sent to the transformer\n",
    "# validation of the data as well\n",
    "# CReate test loop too\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    transformer_model.train()  # Set model to training mode\n",
    "        \n",
    "    total_train_loss = 0\n",
    "    total_train_correct = 0\n",
    "    total_train_samples = 0\n",
    "    \n",
    "    # Get the batch of data from the training split data loader\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) for k, v in batch.items()}\n",
    "\n",
    "        src = batch['input_ids']\n",
    "        labels = batch['label']\n",
    "       \n",
    "        # Forward pass\n",
    "        outputs = transformer_model(src)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        train_loss = loss_fn(outputs, labels)\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "        # Get predicted labels (argmax over logits)\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        \n",
    "        # Calculate accuracy (correct predictions / total samples)\n",
    "        correct_predictions = (predicted_labels == labels).sum().item()\n",
    "        total_train_correct += correct_predictions\n",
    "        total_train_samples += labels.size(0)\n",
    "        \n",
    "        # Backward pass\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate average training loss and accuracy for this epoch\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_accuracy = total_train_correct / total_train_samples\n",
    "\n",
    "    # Now for validation\n",
    "    transformer_model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_samples = 0\n",
    "\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for batch in val_dataloader:\n",
    "            src = batch['input_ids']\n",
    "            labels = batch['label']\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = transformer_model(src)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            val_loss = loss_fn(outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # Get predicted labels (argmax over logits)\n",
    "            _, predicted_labels = torch.max(outputs, dim=1)\n",
    "            \n",
    "            # Calculate accuracy (correct predictions / total samples)\n",
    "            correct_predictions = (predicted_labels == labels).sum().item()\n",
    "            total_val_correct += correct_predictions\n",
    "            total_val_samples += labels.size(0)\n",
    "    \n",
    "    # Calculate average validation loss and accuracy for this epoch\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_accuracy = total_val_correct / total_val_samples\n",
    "\n",
    "    # Store the losses and accuracies\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print epoch stats\n",
    "    print(f'Epoch [{epoch+1}/{epochs}] | '\n",
    "          f'Train Loss: {avg_train_loss:.4f} | Train Accuracy: {train_accuracy*100:.2f}% | '\n",
    "          f'Val Loss: {avg_val_loss:.4f} | Val Accuracy: {val_accuracy*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the training metrics\n",
    "def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plot Training & Validation Loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\", color='orange')\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\", color='red')\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projects_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
